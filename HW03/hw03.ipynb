{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "def process_text(text):\n",
    "    # Split the text into lines and filter out lines starting with </doc> or <doc\n",
    "    lines = text.split('\\n')\n",
    "    filtered_lines = [line for line in lines if not line.startswith('</doc>') and not line.startswith('<doc')]\n",
    "\n",
    "    # Join the filtered lines back into a single string for processing\n",
    "    filtered_text = '\\n'.join(filtered_lines)\n",
    "\n",
    "    processed_sentences = []\n",
    "    sentences = filtered_text.lower().split(\".\")\n",
    "\n",
    "    processed_sentences = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if 'de ' in sentence or 'ki ' in sentence:  # Check if the sentence contains 'de' or 'ki'\n",
    "            words = sentence.split()\n",
    "            processed_words = [word for word in words if not (word.startswith('<') and word.endswith('>'))]\n",
    "            processed_sentence = ' '.join(processed_words).strip()\n",
    "\n",
    "            # Automated labeling based on the presence of ' de ' or ' ki ' as separate words\n",
    "            if ' de ' in processed_sentence or ' ki ' in processed_sentence:\n",
    "                label = '<separated>'\n",
    "            else:\n",
    "                label = '<unified>'  # Default label in case neither is found as separate words\n",
    "\n",
    "            processed_sentences.append(f\"{processed_sentence} {label}\")\n",
    "\n",
    "    return '\\n'.join(processed_sentences)\n",
    "\n",
    "def load_word_embeddings(model_path):\n",
    "    return KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "\n",
    "def convert_to_embeddings(sentences, word_embeddings):\n",
    "    embedded_sentences = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        embedded_sentence = [word_embeddings[word] for word in words if word in word_embeddings]\n",
    "        embedded_sentences.append(embedded_sentence)\n",
    "    return embedded_sentences\n",
    "\n",
    "# Load pre-trained Word2Vec model\n",
    "word_embeddings = load_word_embeddings('/content/drive/MyDrive/Colab/trmodel')\n",
    "\n",
    "def read_partial_file(file_path, portion):\n",
    "    with open(file_path, 'r', encoding='utf8') as file:\n",
    "        file.seek(0, 2)\n",
    "        file_size = file.tell()\n",
    "        file.seek(0)\n",
    "        data_size = int(file_size * portion)\n",
    "        return file.read(data_size)\n",
    "\n",
    "try:\n",
    "    _data = read_partial_file('/content/drive/MyDrive/Colab/wiki_00', 0.3)\n",
    "    training_size = int(len(_data) * 0.95)\n",
    "    training_data = _data[:training_size]\n",
    "    testing_data = _data[training_size:]\n",
    "\n",
    "    processed_training_data = process_text(training_data)\n",
    "    with open('training_data.txt', 'w', encoding=\"utf8\") as f:\n",
    "        f.write(processed_training_data)\n",
    "\n",
    "    processed_testing_data = process_text(testing_data)\n",
    "    with open('testing_data.txt', 'w', encoding=\"utf8\") as f:\n",
    "        f.write(processed_testing_data)\n",
    "\n",
    "    # Split the processed data into individual sentences\n",
    "    processed_sentences = processed_training_data.split('\\n')\n",
    "\n",
    "    # Convert each sentence into word embeddings\n",
    "    embedded_training_data = convert_to_embeddings(processed_sentences, word_embeddings)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dropout, Dense\n",
    "\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "train_sentences, test_sentences = train_test_split(processed_sentences, test_size=0.2, random_state=42)\n",
    "train_sentences, val_sentences = train_test_split(train_sentences, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2\n",
    "\n",
    "# Convert sentences into word embeddings\n",
    "train_data = convert_to_embeddings(train_sentences, word_embeddings)\n",
    "val_data = convert_to_embeddings(val_sentences, word_embeddings)\n",
    "test_data = convert_to_embeddings(test_sentences, word_embeddings)\n",
    "\n",
    "# Convert labels into binary format\n",
    "train_labels = [1 if '<separated>' in sentence else 0 for sentence in train_sentences]\n",
    "val_labels = [1 if '<separated>' in sentence else 0 for sentence in val_sentences]\n",
    "test_labels = [1 if '<separated>' in sentence else 0 for sentence in test_sentences]\n",
    "\n",
    "# Define the shape of the input data\n",
    "input_shape = (100, 300)  # Adjust these numbers to match your data\n",
    "\n",
    "# Define the RNN model with dropout for regularization\n",
    "model = Sequential([\n",
    "    SimpleRNN(50, input_shape=input_shape),\n",
    "    Dropout(0.5),  # Dropout layer with 50% dropout rate\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "yyy\n",
    "\n",
    "# Train the model on the training data and validate on the validation data\n",
    "model.fit(train_data, train_labels, validation_data=(val_data, val_labels), epochs=10)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(test_data, test_labels)\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
