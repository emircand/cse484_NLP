{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Imported libraries which is crucial for the program."
      ],
      "metadata": {
        "id": "67PTiT1u3F6L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "l3SI37cST7be"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calling dataset from google drive"
      ],
      "metadata": {
        "id": "4ismZ5DV5lHo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3CB5hlzz5PVG",
        "outputId": "2753e975-4993-49d7-f7b2-a2b2f7ec8f28"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each sentence is checked for the presence of \" de \" or \" ki \" (with spaces to ensure they are not part of another word). Only sentences containing these substrings are processed and included in the final dataset. This ensures that our dataset focuses on sentences relevant to your task of classifying the usage of \"de\" and \"ki\"."
      ],
      "metadata": {
        "id": "ZjwUO6673Yb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "def process_text(text):\n",
        "    # Split the text into lines and filter out lines starting with </doc> or <doc>\n",
        "    lines = text.split('\\n')\n",
        "    filtered_lines = [line for line in lines if not line.startswith('</doc>') and not line.startswith('<doc')]\n",
        "\n",
        "    # Join the filtered lines back into a single string for processing\n",
        "    filtered_text = '\\n'.join(filtered_lines)\n",
        "\n",
        "    processed_sentences = []\n",
        "    labels = []  # Initialize an empty list to store the labels\n",
        "\n",
        "    sentences = filtered_text.lower().split(\".\")\n",
        "    for sentence in sentences:\n",
        "        if 'de ' in sentence or 'ki ' in sentence:  # Check if the sentence contains 'de' or 'ki'\n",
        "            words = sentence.split()\n",
        "            processed_words = [word for word in words if not (word.startswith('<') and word.endswith('>'))]\n",
        "            processed_sentence = ' '.join(processed_words).strip()\n",
        "\n",
        "            # Determine label based on the presence of ' de ' or ' ki ' as separate words\n",
        "            if ' de ' in processed_sentence or ' ki ' in processed_sentence:\n",
        "                label = 1  # Indicates '<separated>'\n",
        "            else:\n",
        "                label = 0  # Indicates '<unified>'\n",
        "\n",
        "            processed_sentences.append(processed_sentence)\n",
        "            labels.append(label)\n",
        "\n",
        "    return processed_sentences, labels\n",
        "\n",
        "def read_partial_file(file_path, portion):\n",
        "    with open(file_path, 'r', encoding='utf8') as file:\n",
        "        file.seek(0, 2)\n",
        "        file_size = file.tell()\n",
        "        file.seek(0)\n",
        "        data_size = int(file_size * portion)\n",
        "        return file.read(data_size)\n",
        "\n",
        "try:\n",
        "    _data = read_partial_file('/content/drive/MyDrive/Colab/wiki_00', 0.1)\n",
        "    training_size = int(len(_data) * 0.95)\n",
        "    training_data = _data[:training_size]\n",
        "    testing_data = _data[training_size:]\n",
        "\n",
        "    processed_training_data, training_labels = process_text(training_data)\n",
        "    # Convert list to string\n",
        "    processed_training_data_str = '\\n'.join(processed_training_data)\n",
        "    with open('training_data.txt', 'w', encoding=\"utf8\") as f:\n",
        "        f.write(processed_training_data_str)\n",
        "\n",
        "    processed_testing_data, testing_labels = process_text(testing_data)\n",
        "    # Convert list to string\n",
        "    processed_testing_data_str = '\\n'.join(processed_testing_data)\n",
        "    with open('testing_data.txt', 'w', encoding=\"utf8\") as f:\n",
        "        f.write(processed_testing_data_str)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "6uy5tjRCV7ks"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Assuming `processed_training_data` and `processed_testing_data` are lists of sentences\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(processed_training_data)  # Fit tokenizer on training data\n",
        "\n",
        "# Convert sentences to sequences\n",
        "training_sequences = tokenizer.texts_to_sequences(processed_training_data)\n",
        "testing_sequences = tokenizer.texts_to_sequences(processed_testing_data)\n",
        "\n",
        "# Pad sequences to ensure uniform length\n",
        "max_len = max(max(len(x) for x in training_sequences), max(len(x) for x in testing_sequences))\n",
        "X_train = pad_sequences(training_sequences, maxlen=max_len)\n",
        "X_test = pad_sequences(testing_sequences, maxlen=max_len)\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "Y_train = np.array(training_labels)\n",
        "Y_test = np.array(testing_labels)"
      ],
      "metadata": {
        "id": "6CqaR-oNVyTw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manual splitting for validation set\n",
        "val_size = int(len(X_train) * 0.1)  # 10% of training data for validation\n",
        "\n",
        "X_val = X_train[:val_size]\n",
        "Y_val = Y_train[:val_size]\n",
        "\n",
        "X_train_new = X_train[val_size:]\n",
        "Y_train_new = Y_train[val_size:]"
      ],
      "metadata": {
        "id": "EjqP1pHXDO-F"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Assuming tokenizer is already fitted to your corpus\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=100, input_length=max_len),  # Increased embedding dimension\n",
        "    LSTM(64),  # Using LSTM layer to capture sequential dependencies\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer and binary crossentropy loss\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "ykX_agYJV1x7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "#early stopping to halt the training when the validation loss stops improving. This can save time by preventing unnecessary epochs.\n",
        "#early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train_new, Y_train_new,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    validation_data=(X_val, Y_val),\n",
        "    verbose=1\n",
        "    #callbacks=[early_stopping]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbDyXoQoV3ri",
        "outputId": "7dd1fd44-8c56-4b46-be45-7d012f656aa9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4835/4835 [==============================] - 156s 32ms/step - loss: 3.6199e-05 - accuracy: 1.0000 - val_loss: 0.1101 - val_accuracy: 0.9905\n",
            "Epoch 2/10\n",
            "4835/4835 [==============================] - 154s 32ms/step - loss: 3.3135e-06 - accuracy: 1.0000 - val_loss: 0.1463 - val_accuracy: 0.9864\n",
            "Epoch 3/10\n",
            "4835/4835 [==============================] - 156s 32ms/step - loss: 2.5517e-08 - accuracy: 1.0000 - val_loss: 0.1573 - val_accuracy: 0.9870\n",
            "Epoch 4/10\n",
            "4835/4835 [==============================] - 156s 32ms/step - loss: 3.6695e-09 - accuracy: 1.0000 - val_loss: 0.1680 - val_accuracy: 0.9874\n",
            "Epoch 5/10\n",
            "4835/4835 [==============================] - 158s 33ms/step - loss: 7.8013e-10 - accuracy: 1.0000 - val_loss: 0.1765 - val_accuracy: 0.9878\n",
            "Epoch 6/10\n",
            "4835/4835 [==============================] - 157s 32ms/step - loss: 2.9483e-10 - accuracy: 1.0000 - val_loss: 0.1824 - val_accuracy: 0.9879\n",
            "Epoch 7/10\n",
            "4835/4835 [==============================] - 155s 32ms/step - loss: 1.7490e-10 - accuracy: 1.0000 - val_loss: 0.1860 - val_accuracy: 0.9880\n",
            "Epoch 8/10\n",
            "4835/4835 [==============================] - 155s 32ms/step - loss: 1.2439e-10 - accuracy: 1.0000 - val_loss: 0.1886 - val_accuracy: 0.9880\n",
            "Epoch 9/10\n",
            "4835/4835 [==============================] - 154s 32ms/step - loss: 9.7778e-11 - accuracy: 1.0000 - val_loss: 0.1907 - val_accuracy: 0.9880\n",
            "Epoch 10/10\n",
            "4835/4835 [==============================] - 154s 32ms/step - loss: 8.0345e-11 - accuracy: 1.0000 - val_loss: 0.1923 - val_accuracy: 0.9879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the test sentences\n",
        "test_sequences = tokenizer.texts_to_sequences(processed_testing_data)  # Assuming this is your list of test sentences\n",
        "\n",
        "# Pad the sequences\n",
        "X_test = pad_sequences(test_sequences, maxlen=max_len)\n",
        "\n",
        "# Ensure the test labels are in the correct format\n",
        "Y_test = np.array(testing_labels)\n",
        "\n",
        "evaluation_results = model.evaluate(X_test, Y_test)\n",
        "print(evaluation_results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_S09OCcV5LZ",
        "outputId": "deb2174d-415e-43c7-a73d-4a8df9ab80c4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "276/276 [==============================] - 4s 13ms/step - loss: 0.1822 - accuracy: 0.9882\n",
            "[0.18216589093208313, 0.9882139563560486]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Test sentences\n",
        "test_sentences = [\n",
        "    \"Kitap masanın üstünde duruyordu.\",\n",
        "    \"Arkadaşlar da gelmiş.\",\n",
        "    \"O günki hava çok güzeldi.\",\n",
        "    \"Derslerinde başarılı bir öğrenciydi.\",\n",
        "    \"Herkesin de bir hikayesi var.\",\n",
        "    \"Kapıdaki kimdi?\",\n",
        "    \"Yazdıklarını okudum da çok beğendim.\",\n",
        "    \"Bu işin sonu nereye varacak bilmiyorum ki.\",\n",
        "    \"Olanlar olmuştu artık, yapacak bir şey yoktu.\",\n",
        "    \"Sen de mi Brutus?\",\n",
        "    \"Annesi de buradaydı.\",\n",
        "    \"Bize de haber verir misin?\",\n",
        "    \"Kitaplarda masanın üstündeydi.\",\n",
        "    \"Yemekler hazırlandı ki.\",\n",
        "    \"Evimizde duruyor.\",\n",
        "    \"O günde çok güzeldi.\",\n",
        "    \"Yarın da gelecekler.\",\n",
        "    \"Gömlekteki leke çıktı.\",\n",
        "    \"Dün de aynıydı.\",\n",
        "    \"Herkesinki farklı\"\n",
        "]\n",
        "\n",
        "# Predict whether \"de\" and \"ki\" suffixes should be separated or not for each sentence\n",
        "for input_text in test_sentences:\n",
        "    # Tokenize the input text\n",
        "    sequence = tokenizer.texts_to_sequences([input_text])\n",
        "    # Pad the sequence\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=max_len)\n",
        "\n",
        "    # Use model.predict\n",
        "    prediction = model.predict(padded_sequence)\n",
        "    # Convert probability to class label\n",
        "    class_prediction = (prediction > 0.5).astype(int)\n",
        "\n",
        "    print(f\"Sentence: '{input_text}'\")\n",
        "    print(f\"Prediction: {'Should be separated' if class_prediction[0][0] == 1 else 'Should be unified'}\")\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhJZ7RAa_MyB",
        "outputId": "b6c77534-c46e-44ba-b57d-9b08d6d5de63"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 31ms/step\n",
            "Sentence: 'Kitap masanın üstünde duruyordu.'\n",
            "Prediction: Should be unified\n",
            "\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Sentence: 'Arkadaşlar da gelmiş.'\n",
            "Prediction: Should be unified\n",
            "\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Sentence: 'O günki hava çok güzeldi.'\n",
            "Prediction: Should be unified\n",
            "\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Sentence: 'Derslerinde başarılı bir öğrenciydi.'\n",
            "Prediction: Should be unified\n",
            "\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Sentence: 'Herkesin de bir hikayesi var.'\n",
            "Prediction: Should be separated\n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Sentence: 'Kapıdaki kimdi?'\n",
            "Prediction: Should be unified\n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Sentence: 'Yazdıklarını okudum da çok beğendim.'\n",
            "Prediction: Should be unified\n",
            "\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Sentence: 'Bu işin sonu nereye varacak bilmiyorum ki.'\n",
            "Prediction: Should be unified\n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Sentence: 'Olanlar olmuştu artık, yapacak bir şey yoktu.'\n",
            "Prediction: Should be unified\n",
            "\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Sentence: 'Sen de mi Brutus?'\n",
            "Prediction: Should be separated\n",
            "\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "Sentence: 'Annesi de buradaydı.'\n",
            "Prediction: Should be separated\n",
            "\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "Sentence: 'Bize de haber verir misin?'\n",
            "Prediction: Should be separated\n",
            "\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "Sentence: 'Kitaplarda masanın üstündeydi.'\n",
            "Prediction: Should be unified\n",
            "\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Sentence: 'Yemekler hazırlandı ki.'\n",
            "Prediction: Should be unified\n",
            "\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "Sentence: 'Evimizde duruyor.'\n",
            "Prediction: Should be unified\n",
            "\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "Sentence: 'O günde çok güzeldi.'\n",
            "Prediction: Should be unified\n",
            "\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Sentence: 'Yarın da gelecekler.'\n",
            "Prediction: Should be unified\n",
            "\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Sentence: 'Gömlekteki leke çıktı.'\n",
            "Prediction: Should be unified\n",
            "\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Sentence: 'Dün de aynıydı.'\n",
            "Prediction: Should be separated\n",
            "\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "Sentence: 'Herkesinki farklı'\n",
            "Prediction: Should be unified\n",
            "\n"
          ]
        }
      ]
    }
  ]
}