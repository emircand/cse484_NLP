{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf7f9a1-14a7-4188-911c-3fc048618445",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install git+https://github.com/ftkurt/python-syllable.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a1532e1-4eba-443e-a793-a02014f43757",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from syllable import Encoder\n",
    "\n",
    "encoder = Encoder(lang=\"tr\", limitby=\"vocabulary\", limit=100000000000)\n",
    "\n",
    "def process_text(text):\n",
    "    replacements = {\"ı\": \"i\", \"ö\": \"o\", \"ğ\": \"g\", \"ç\": \"c\", \"â\": \"a\", \"ş\": \"s\", \"ü\": \"u\"}\n",
    "    pattern = re.compile(\"|\".join(replacements.keys()))\n",
    "\n",
    "    def replace_chars(match):\n",
    "        return replacements[match.group(0)]\n",
    "\n",
    "    # Split the text into lines and filter out lines starting with </doc> or <doc\n",
    "    lines = text.split('\\n')\n",
    "    filtered_lines = [line for line in lines if not line.startswith('</doc>') and not line.startswith('<doc')]\n",
    "\n",
    "    # Join the filtered lines back into a single string for processing\n",
    "    filtered_text = '\\n'.join(filtered_lines)\n",
    "\n",
    "    processed_sentences = []\n",
    "    sentences = filtered_text.lower().split(\".\")\n",
    "\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        words_with_spc = []\n",
    "        for word in words:\n",
    "            if word.startswith('<') and word.endswith('>'):\n",
    "                continue\n",
    "                \n",
    "            syllables = encoder.tokenize(word)\n",
    "            processed_syllables = [pattern.sub(replace_chars, syllable) for syllable in syllables]\n",
    "            processed_word = ''.join(processed_syllables)\n",
    "            words_with_spc.extend([processed_word, \"<space>\"])\n",
    "\n",
    "        processed_sentence = ' '.join(words_with_spc).strip()\n",
    "        processed_sentences.append(processed_sentence)\n",
    "\n",
    "    return '. '.join(processed_sentences)\n",
    "\n",
    "def read_partial_file(file_path, portion):\n",
    "    with open(file_path, 'r', encoding='utf8') as file:\n",
    "        file.seek(0, 2)\n",
    "        file_size = file.tell()\n",
    "        file.seek(0)\n",
    "        data_size = int(file_size * portion)\n",
    "        return file.read(data_size)\n",
    "\n",
    "try:\n",
    "    twenty_percent_data = read_partial_file('wiki_00', 0.20)\n",
    "    training_size = int(len(twenty_percent_data) * 0.95)\n",
    "    training_data = twenty_percent_data[:training_size]\n",
    "    testing_data = twenty_percent_data[training_size:]\n",
    "\n",
    "    processed_training_data = process_text(training_data)\n",
    "    with open('training_data', 'w', encoding=\"utf8\") as f:\n",
    "        f.write(processed_training_data)\n",
    "\n",
    "    processed_testing_data = process_text(testing_data)\n",
    "    with open('testing_data', 'w', encoding=\"utf8\") as f:\n",
    "        f.write(processed_testing_data)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1af08868-1ec5-4482-a74d-5ed4c44a74a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import product\n",
    "\n",
    "def unigram(txt):\n",
    "    \"\"\"Unigram dictionary builder using Counter with <SOS> and <EOS> tags\"\"\"\n",
    "    # Split the text into syllables\n",
    "    raw_syllables = txt.split()\n",
    "\n",
    "    # Process syllables to remove periods and add <EOS> and <SOS> tags\n",
    "    syllables = []\n",
    "    syllables.append(\"<SOS>\") # Add <SOS> tag at the beginning of the corpus (depended to the corpus)\n",
    "    for i, syllable in enumerate(raw_syllables):\n",
    "        if syllable.endswith('.'):\n",
    "            syllables.append(syllable[:-1])  # Remove period\n",
    "            syllables.append(\"<EOS>\")\n",
    "            if i < len(raw_syllables) - 1:   # Add <SOS> if not the last syllable in the text\n",
    "                syllables.append(\"<SOS>\")\n",
    "        else:\n",
    "            syllables.append(syllable)\n",
    "\n",
    "    # Use Counter to count occurrences of each syllable\n",
    "    uni_freq = Counter(syllables)\n",
    "    syllables_count = len(syllables)\n",
    "    return uni_freq, syllables_count\n",
    "\n",
    "def bigram(txt, uni_freq):\n",
    "    \"\"\"Calculate and store probabilities for each bigram prefix, excluding zero cases.\"\"\"\n",
    "    # Extract unique syllables from unigram frequencies\n",
    "    unique_syllables = list(uni_freq.keys())\n",
    "\n",
    "    # Create all possible bigram combinations\n",
    "    all_bigrams = list(product(unique_syllables, repeat=2))\n",
    "\n",
    "    # Initialize bigram counts\n",
    "    bi_counts = {bigram: 0 for bigram in all_bigrams}\n",
    "\n",
    "    # Split the text into syllables and tag <EOS> and <SOS>\n",
    "    syllables = txt.split()\n",
    "    tagged_syllables = [\"<SOS>\"]\n",
    "    for i, syllable in enumerate(syllables):\n",
    "        if syllable.endswith('.'):\n",
    "            tagged_syllables.append(syllable[:-1])\n",
    "            tagged_syllables.append(\"<EOS>\")\n",
    "            if i < len(syllables) - 1:\n",
    "                tagged_syllables.append(\"<SOS>\")\n",
    "        else:\n",
    "            tagged_syllables.append(syllable)\n",
    "\n",
    "    # Count actual occurrences of each bigram\n",
    "    for i in range(len(tagged_syllables) - 1):\n",
    "        bigram = (tagged_syllables[i], tagged_syllables[i + 1])\n",
    "        bi_counts[bigram] += 1\n",
    "    # Convert counts to probabilities and exclude zero cases\n",
    "    bi_prob = {}\n",
    "    for bigram in bi_counts:\n",
    "        prefix = bigram[0]\n",
    "        if prefix in uni_freq and uni_freq[prefix] > 0:\n",
    "            probability = bi_counts[bigram] / uni_freq[prefix]\n",
    "            if probability > 0:\n",
    "                bi_prob[bigram] = probability\n",
    "    return bi_prob\n",
    "\n",
    "def trigram(txt, uni_freq):\n",
    "    \"\"\"Calculate and store probabilities for each trigram prefix, excluding zero cases, in a memory-efficient way.\"\"\"\n",
    "    # Initialize dictionaries for bigram counts and trigram counts\n",
    "    bi_counts = {}\n",
    "    tri_counts = {}\n",
    "\n",
    "    # Split the text into syllables and tag <EOS> and <SOS>\n",
    "    syllables = ['<SOS>'] + [syl[:-1] if syl.endswith('.') else syl for syl in txt.split()]\n",
    "    syllables += ['<EOS>' if syllables[-1] != '<SOS>' else '']\n",
    "\n",
    "    # Count occurrences of each bigram and trigram\n",
    "    for i in range(len(syllables) - 2):\n",
    "        bigram = (syllables[i], syllables[i + 1])\n",
    "        trigram = (syllables[i], syllables[i + 1], syllables[i + 2])\n",
    "\n",
    "        # Increment bigram and trigram counts\n",
    "        bi_counts[bigram] = bi_counts.get(bigram, 0) + 1\n",
    "        tri_counts[trigram] = tri_counts.get(trigram, 0) + 1\n",
    "\n",
    "    # Convert trigram counts to probabilities\n",
    "    tri_prob = {}\n",
    "    for trigram, count in tri_counts.items():\n",
    "        prefix = (trigram[0], trigram[1])\n",
    "        if prefix in bi_counts:\n",
    "            probability = count / bi_counts[prefix]\n",
    "            if probability > 0:\n",
    "                tri_prob[trigram] = probability\n",
    "\n",
    "    return tri_prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bb224f3-a77c-4df5-b00d-ab0ccc928b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram progress done and written to the file...\n",
      "bigram progress done and written to the file...\n",
      "trigram progress done and written to the file...\n"
     ]
    }
   ],
   "source": [
    "uni_freq, syllables_count = unigram(processed_training_data)\n",
    "# Writing unigram frequencies to a file\n",
    "with open('unigram_results.txt', 'w', encoding='utf-8') as file:\n",
    "    for syllable, frequency in uni_freq.items():\n",
    "        file.write(f\"{syllable}: {frequency}\\n\")\n",
    "print(\"unigram progress done and written to the file...\")\n",
    "bi_prob = bigram(processed_training_data, uni_freq)\n",
    "# Writing bigram probabilities to a file\n",
    "with open('bigram_results.txt', 'w', encoding='utf-8') as file:\n",
    "    for bigram, probability in bi_prob.items():\n",
    "        file.write(f\"{bigram}: {probability}\\n\")\n",
    "print(\"bigram progress done and written to the file...\")\n",
    "tri_prob = trigram(processed_training_data, uni_freq)\n",
    "# Writing bigram probabilities to a file\n",
    "with open('trigram_results.txt', 'w', encoding='utf-8') as file:\n",
    "    for trigram, probability in tri_prob.items():\n",
    "        file.write(f\"{trigram}: {probability}\\n\")\n",
    "print(\"trigram progress done and written to the file...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3020677c-a983-4892-b951-1b7ac4f0c066",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "def generate_sentence_from_unigram(txt, uni_freq):\n",
    "    \"\"\"Generate a random sentence using the unigram model\"\"\"\n",
    "    # Sort syllables by frequency, excluding <SOS>, <EOS>, and <space>\n",
    "    sorted_syllables = [syl for syl in sorted(uni_freq, key=uni_freq.get, reverse=True) if syl not in [\"<SOS>\", \"<EOS>\", \"<space>\"]]\n",
    "\n",
    "    # Pick top 5 syllables\n",
    "    top_5_syllables = sorted_syllables[:5]\n",
    "\n",
    "    # Start sentence generation\n",
    "    sentence = [\"<SOS>\"]\n",
    "    i = 0\n",
    "    while i<5:\n",
    "        # Randomly pick one of the top 5 syllables\n",
    "        next_syllable = random.choice(top_5_syllables)\n",
    "        i += 1\n",
    "        \n",
    "        # Break if <EOS> is chosen\n",
    "        if next_syllable == \"<EOS>\":\n",
    "            break\n",
    "\n",
    "        # Add syllable to sentence\n",
    "        sentence.append(next_syllable)\n",
    "    \n",
    "    # Return the generated sentence as a string\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "def generate_sentence_from_bigram(txt, uni_freq, bi_prob, max_length=30):\n",
    "    \"\"\"Generate a random sentence using the bigram model\"\"\"\n",
    "    sentence = [\"<SOS>\"]\n",
    "    while len(sentence) < max_length + 1:\n",
    "        last_syllable = sentence[-1]\n",
    "\n",
    "        # Get bigrams that start with the last syllable\n",
    "        next_syllables = [(pair, prob) for pair, prob in bi_prob.items() if pair[0] == last_syllable]\n",
    "        next_syllables = sorted(next_syllables, key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "        if next_syllables:\n",
    "            next_syllable = random.choice(next_syllables)[0][1]\n",
    "        else:  # Fallback to unigram model\n",
    "            sorted_syllables = [syl for syl in sorted(uni_freq, key=uni_freq.get, reverse=True) if syl not in [\"<SOS>\", \"<EOS>\", \"<space>\"]]\n",
    "            next_syllable = random.choice(sorted_syllables)\n",
    "\n",
    "        if next_syllable == \"<EOS>\":\n",
    "            break\n",
    "        sentence.append(next_syllable)\n",
    "\n",
    "    return ' '.join(sentence[1:])\n",
    "\n",
    "def generate_sentence_from_trigram(txt, uni_freq, bi_prob, tri_prob, max_length=30):\n",
    "    \"\"\"Generate a random sentence using the trigram model\"\"\"\n",
    "    sentence = [\"<SOS>\"]\n",
    "    while len(sentence) < max_length + 1:\n",
    "        last_two_syllables = tuple(sentence[-2:])\n",
    "        next_syllables = [(triplet, prob) for triplet, prob in tri_prob.items() if triplet[:2] == last_two_syllables]\n",
    "        next_syllables = sorted(next_syllables, key=lambda x: x[1], reverse=True)[:5]\n",
    "\n",
    "        if next_syllables:\n",
    "            next_syllable = random.choice(next_syllables)[0][2]\n",
    "        else:  # Fallback to bigram model\n",
    "            last_syllable = sentence[-1]\n",
    "            next_syllables = [(bigram, prob) for bigram, prob in bi_prob.items() if bigram[0] == last_syllable]\n",
    "            next_syllables = sorted(next_syllables, key=lambda x: x[1], reverse=True)[:5]\n",
    "            if next_syllables:\n",
    "                next_syllable = random.choice(next_syllables)[0][1]\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        if next_syllable == \"<EOS>\":\n",
    "            break\n",
    "        sentence.append(next_syllable)\n",
    "\n",
    "    return ' '.join(sentence[1:])\n",
    "    \n",
    "def concatenate_words(word_list):\n",
    "    result = ''\n",
    "    for word in word_list:\n",
    "        if word == '<space>' or word == '<SOS>':\n",
    "            result += ' '  # Add a space whenever '<space>' is encountered\n",
    "        elif word == '<EOS>':\n",
    "            result += '.'\n",
    "        else:\n",
    "            result += word  # Concatenate the word\n",
    "    return result.strip()  # Remove any leading or trailing spaces\n",
    "\n",
    "def print_generated_sentence(txt, model_type):\n",
    "    if model_type == 'unigram':\n",
    "        sentence_list = generate_sentence_from_unigram(txt, uni_freq)\n",
    "    elif model_type == 'bigram':\n",
    "        sentence_list = generate_sentence_from_bigram(txt, uni_freq, bi_prob)\n",
    "    elif model_type == 'trigram':\n",
    "        sentence_list = generate_sentence_from_trigram(txt, uni_freq, bi_prob, tri_prob)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model type. Choose 'unigram', 'bigram', or 'trigram'.\")\n",
    "\n",
    "    # Format the sentence\n",
    "    formatted_sentence = concatenate_words(sentence_list.split())\n",
    "    print(f\"{formatted_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5874ef95-a0e0-47e1-9486-da215476f9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Model Generated Sentences:\n",
      "ladaridari\n",
      "sirilalasi\n",
      "laridasila\n",
      "rilasisisi\n",
      "lalalerida\n",
      "ririsisisi\n",
      "ridalasila\n",
      "lelasilesi\n",
      "ridasilada\n",
      "lesilasile\n",
      "\n",
      "Bigram Model Generated Sentences:\n",
      "onemler i ve ve a ozelliginilirlerlerinilantigiliginindansayiline o\n",
      "bu iki\n",
      "bu ile olanti verilme olari verilmesininmistirinabi ve ozellikteleri\n",
      "onem verilmadiyerinadi ikimi\n",
      "buyukseldigercektirdisi adisina\n",
      "buyuklugunetiginin ve olarak\n",
      "i verini a\n",
      "a verenlerindenlerdenge icinsel\n",
      "o ikiminadiyerinindahavasi\n",
      "o ve\n",
      "\n",
      "Trigram Model Generated Sentences:\n",
      "arazisi ve analizmin ettigi ozelliklerini olanlarlaridirlerhavaliderilerinizininkilerininde bununda\n",
      "bu icindeyken ikiye baslanmistisiz birlikte alan i  tarihlidirlerdirmesiniridirdemokritostur alanyayabanil\n",
      "adini iselerden sonsuzluklarindadaciligiyle   tarafindaysa edeme iki birlikte aracigerlerdenim veri\n",
      "arasidirlerde yapilmissatilmasin anaya sayila     araci verilen  tarih kuzey alan adinin o\n",
      "alan adinatorlu ozelde olacaktiriciliginindapordada alanlar ve alanla araliksizdi  iki ana\n",
      "arazinindakikalik  iselerdendir ikiyeyihi ilenine kabul ediyormussu  yilin baslangi  yikildigi\n",
      "ozellikleri   ikili icinde ozel ilenine alanilirlarmiscasinabilimsellikolumcul de   yikimininki\n",
      "i iseinfeldla ikiserligezisinindayim isei veyahut dahildirlerdiringilo i alan ikiyeye arasi\n",
      "bulunmasi veren alanlariylaformation  alanlarla i duncandacenap lisans ogrendi analinemeyerekten sonra\n",
      "i  tasimasi olabi olmak icindeydim isteginindekilerdeki bulundukten sonbaharda yasamimi onemsiz o\n"
     ]
    }
   ],
   "source": [
    "print(\"Unigram Model Generated Sentences:\")\n",
    "for i in range(10):\n",
    "    print(f\"sentence {i}:\" )\n",
    "    print_generated_sentence(processed_training_data, 'unigram')\n",
    "\n",
    "# Calling the bigram model 10 times\n",
    "print(\"\\nBigram Model Generated Sentences:\")\n",
    "for i in range(10):\n",
    "    print(f\"sentence {i}:\" )\n",
    "    print_generated_sentence(processed_training_data, 'bigram')\n",
    "\n",
    "# Calling the trigram model 10 times\n",
    "print(\"\\nTrigram Model Generated Sentences:\")\n",
    "for i in range(10):\n",
    "    print(f\"sentence {i}:\" )\n",
    "    print_generated_sentence(processed_training_data, 'trigram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e6d4d80-47a6-415d-af24-b0e7773123d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram perplexity result = 129.16566329712467\n",
      "bigram perplexity result = 28.0135892981899\n",
      "trigram perplexity result = 11.775844697831047\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def calculate_unigram_perplexity(uni_prob, test_syllables):\n",
    "    \"\"\"Calculate perplexity for unigram model.\"\"\"\n",
    "    log_sum = 0\n",
    "    uni_count = 0\n",
    "\n",
    "    # Iterate through the test syllables\n",
    "    for syllable in test_syllables:\n",
    "        if syllable in uni_prob:\n",
    "            probability = uni_prob[syllable]\n",
    "            log_sum += math.log2(probability)\n",
    "            uni_count += 1\n",
    "\n",
    "    # Calculate perplexity\n",
    "    if uni_count > 0:\n",
    "        perplexity = 2 ** (-log_sum / uni_count)\n",
    "    else:\n",
    "        perplexity = float('inf')  # Set to infinity if no unigram found\n",
    "\n",
    "    return perplexity\n",
    "\n",
    "def calculate_bigram_perplexity(bi_prob, test_syllables):\n",
    "    \"\"\"Calculate perplexity for bigram model.\"\"\"\n",
    "    log_sum = 0\n",
    "    bi_count = 0\n",
    "\n",
    "    # Iterate through the test syllables for bigrams\n",
    "    for i in range(len(test_syllables) - 1):\n",
    "        bigram = (test_syllables[i], test_syllables[i + 1])\n",
    "        if bigram in bi_prob:\n",
    "            probability = bi_prob[bigram]\n",
    "            log_sum += math.log2(probability)\n",
    "            bi_count += 1\n",
    "\n",
    "    # Calculate perplexity\n",
    "    if bi_count > 0:\n",
    "        perplexity = 2 ** (-log_sum / bi_count)\n",
    "    else:\n",
    "        perplexity = float('inf')  # Set to infinity if no bigram found\n",
    "\n",
    "    return perplexity\n",
    "\n",
    "def calculate_trigram_perplexity(tri_prob, test_syllables):\n",
    "    \"\"\"Calculate perplexity for trigram model.\"\"\"\n",
    "    # Initial variables\n",
    "    log_sum = 0\n",
    "    tri_count = 0\n",
    "\n",
    "    # Iterate through the test syllables for trigrams\n",
    "    for i in range(len(test_syllables) - 2):\n",
    "        trigram = (test_syllables[i], test_syllables[i + 1], test_syllables[i + 2])\n",
    "        if trigram in tri_prob:\n",
    "            probability = tri_prob[trigram]\n",
    "            log_sum += math.log2(probability)\n",
    "            tri_count += 1\n",
    "\n",
    "    # Calculate perplexity\n",
    "    if tri_count > 0:\n",
    "        perplexity = 2 ** (-log_sum / tri_count)\n",
    "    else:\n",
    "        perplexity = float('inf')  # Set to infinity if no trigram found\n",
    "\n",
    "    return perplexity\n",
    "\n",
    "training_text = processed_training_data\n",
    "test_text = processed_testing_data\n",
    "# Convert frequency to probability\n",
    "unigram_probabilities = {word: freq / syllables_count for word, freq in uni_freq.items()}\n",
    "\n",
    "# Prepare test data\n",
    "test_words = test_text.split()\n",
    "test_words = [\"<SOS>\"] + test_words  # Add <SOS> tag for bigrams\n",
    "test_words.append(\"<EOS>\")  # Add <EOS> tag for bigrams\n",
    "\n",
    "# Calculate perplexity\n",
    "unigram_perplexity = calculate_unigram_perplexity(unigram_probabilities, test_words)\n",
    "print(\"unigram perplexity result =\", unigram_perplexity)\n",
    "\n",
    "# Calculate bigram perplexity\n",
    "bigram_perplexity = calculate_bigram_perplexity(bi_prob, test_words)\n",
    "print(\"bigram perplexity result =\", bigram_perplexity)\n",
    "\n",
    "# Calculate trigram perplexity\n",
    "trigram_perplexity = calculate_trigram_perplexity(tri_prob, test_words)\n",
    "print(\"trigram perplexity result =\", trigram_perplexity)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
